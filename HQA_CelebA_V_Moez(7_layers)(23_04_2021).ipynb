{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HQA_CelebA_V_Moez(7 layers)(23_04_2021).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moez-RT/HQA/blob/main/HQA_CelebA_V_Moez(7_layers)(23_04_2021).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8lCFMMQotuc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk_aUuh4pPeD"
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import types\n",
        "import datetime\n",
        "import random\n",
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, distributions\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.autograd import Function\n",
        "from torch.distributions import RelaxedOneHotCategorical, Normal, Categorical\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CelebA \n",
        "from torchvision.utils import make_grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY7HXNXppPed"
      },
      "source": [
        "def set_seeds(seed=42, fully_deterministic=False):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "    if fully_deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seeds()\n",
        "\n",
        "print(f\"CUDA={torch.cuda.is_available()}\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkXkrSZFpPee"
      },
      "source": [
        "### CelebA Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNSfXAlHZ0zl"
      },
      "source": [
        "!rm -rf data\n",
        "!rm -rf models\n",
        "import os\n",
        "import zipfile \n",
        "import gdown\n",
        "import torch\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "data_root = 'data/celeba'\n",
        "# Path to folder with the dataset\n",
        "dataset_folder = f'{data_root}/img_align_celeba'\n",
        "# URL for the CelebA dataset\n",
        "url = 'https://drive.google.com/uc?id=1cNIac61PSA_LqDFYFUeyaQYekYPc75NH'\n",
        "# Path to download the dataset to\n",
        "download_path = f'{data_root}/img_align_celeba.zip'\n",
        "\n",
        "# Create required directories \n",
        "if not os.path.exists(data_root):\n",
        "  os.makedirs(data_root)\n",
        "  os.makedirs(dataset_folder)\n",
        "\n",
        "# Download the dataset from google drive\n",
        "gdown.download(url, download_path, quiet=False)\n",
        "\n",
        "# Unzip the downloaded file \n",
        "with zipfile.ZipFile(download_path, 'r') as ziphandler:\n",
        "  ziphandler.extractall(dataset_folder)\n",
        "\n",
        "## Create a custom Dataset class\n",
        "class CelebADataset(Dataset):\n",
        "  def __init__(self, root_dir, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      root_dir (string): Directory with all the images\n",
        "      transform (callable, optional): transform to be applied to each image sample\n",
        "    \"\"\"\n",
        "    # Read names of images in the root directory\n",
        "    image_names = os.listdir(root_dir)\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform \n",
        "    self.image_names = natsorted(image_names)\n",
        "\n",
        "  def __len__(self): \n",
        "    return len(self.image_names)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get the path to the image \n",
        "    img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
        "    # Load image and convert it to RGB\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    # Apply transformations to the image\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irCXkE_n2Yb-"
      },
      "source": [
        "\n",
        "## Load the dataset \n",
        "# Path to directory with all the images\n",
        "img_folder = f'{dataset_folder}/img_align_celeba'\n",
        "# Spatial size of training images, images are resized to this size.\n",
        "image_size = 64\n",
        "# Transformations to be applied to each individual image sample\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "\n",
        "  \n",
        "])\n",
        "# Load the dataset from file and apply transformations\n",
        "celeba_dataset = CelebADataset(img_folder, transform)\n",
        "\n",
        "## Create a dataloader \n",
        "# Batch size during training\n",
        "batch_size = 1024\n",
        "# Number of workers for the dataloader\n",
        "num_workers = 0 if device.type == 'cuda' else 2\n",
        "# Whether to put fetched data tensors to pinned memory\n",
        "pin_memory = True if device.type == 'cuda' else False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHJgTowBaAIY"
      },
      "source": [
        "\n",
        "len_dataset = int(5e3)\n",
        "\n",
        "part_tr = torch.utils.data.random_split(celeba_dataset, [len_dataset, len(celeba_dataset) - len_dataset], generator=torch.Generator().manual_seed(42))[0]\n",
        "len(part_tr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCB-YAEFpPef"
      },
      "source": [
        "\n",
        "dl_train = torch.utils.data.DataLoader(part_tr,\n",
        "                                                batch_size=batch_size,\n",
        "                                                num_workers=num_workers,\n",
        "                                                pin_memory=pin_memory,\n",
        "                                                shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YNRMQQIz-o7"
      },
      "source": [
        "test_x = next(iter(dl_train))\n",
        "test_x = test_x.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSDTGLHsFIaq"
      },
      "source": [
        "z = None\n",
        "for x  in dl_train:\n",
        "  z = x[0]\n",
        "  print(x[0].shape)\n",
        "  print(len(x))\n",
        "  break\n",
        "y= z.permute(1,2,0).contiguous()\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qsQnvoZ-sKy"
      },
      "source": [
        "z[:].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9MXjj5at-2q"
      },
      "source": [
        "plt.imshow(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1mRs_GQpPeg"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxVZmDd5pPeg"
      },
      "source": [
        "\n",
        "def double_swap(img):\n",
        "  img = np.swapaxes(img, 0,-1)\n",
        "  img = np.swapaxes(img, 0,1)\n",
        "  return img\n",
        "\n",
        "def show_image(im_data, scale=10):\n",
        "    dpi = matplotlib.rcParams['figure.dpi']\n",
        "    width, height ,channel = im_data.shape\n",
        "    figsize = scale * width / float(dpi), scale * height / float(dpi)\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = fig.add_axes([0, 0, 1, 1])\n",
        "    # Hide spines, ticks, etc.\n",
        "    ax.axis('off')\n",
        "    ax.imshow(im_data, vmin=0, vmax=1)\n",
        "    plt.show();\n",
        "    ax.set(xlim=[0, width], ylim=[height, 0], aspect=1)\n",
        "\n",
        "\n",
        "class RAdam(Optimizer):\n",
        "    def __init__(\n",
        "        self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True\n",
        "    ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "\n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if \"betas\" in param and (\n",
        "                    param[\"betas\"][0] != betas[0] or param[\"betas\"][1] != betas[1]\n",
        "                ):\n",
        "                    param[\"buffer\"] = [[None, None, None] for _ in range(10)]\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            betas=betas,\n",
        "            eps=eps,\n",
        "            weight_decay=weight_decay,\n",
        "            buffer=[[None, None, None] for _ in range(10)],\n",
        "        )\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                buffered = group[\"buffer\"][int(state[\"step\"] % 10)]\n",
        "                if state[\"step\"] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state[\"step\"]\n",
        "                    beta2_t = beta2 ** state[\"step\"]\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = math.sqrt(\n",
        "                            (1 - beta2_t)\n",
        "                            * (N_sma - 4)\n",
        "                            / (N_sma_max - 4)\n",
        "                            * (N_sma - 2)\n",
        "                            / N_sma\n",
        "                            * N_sma_max\n",
        "                            / (N_sma_max - 2)\n",
        "                        ) / (1 - beta1 ** state[\"step\"])\n",
        "                    elif self.degenerated_to_sgd:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n",
        "                    else:\n",
        "                        step_size = -1\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group[\"weight_decay\"] != 0:\n",
        "                        p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group[\"lr\"], exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif step_size > 0:\n",
        "                    if group[\"weight_decay\"] != 0:\n",
        "                        p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n",
        "                    p_data_fp32.add_(-step_size * group[\"lr\"], exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Mish - \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\"\n",
        "# https://arxiv.org/abs/1908.08681v1\n",
        "# implemented for PyTorch / FastAI by lessw2020\n",
        "# github: https://github.com/lessw2020/mish\n",
        "def mish(x):\n",
        "    return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class Mish(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return mish(x)\n",
        "    \n",
        "# https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html\n",
        "class FlatCA(_LRScheduler):\n",
        "    def __init__(self, optimizer, steps, eta_min=0, last_epoch=-1):\n",
        "        self.steps = steps\n",
        "        self.eta_min = eta_min\n",
        "        super(FlatCA, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr_list = []\n",
        "        T_max = self.steps / 3\n",
        "        for base_lr in self.base_lrs:\n",
        "            # flat if first 2/3\n",
        "            if 0 <= self._step_count < 2 * T_max:\n",
        "                lr_list.append(base_lr)\n",
        "            # annealed if last 1/3\n",
        "            else:\n",
        "                lr_list.append(\n",
        "                    self.eta_min\n",
        "                    + (base_lr - self.eta_min)\n",
        "                    * (1 + math.cos(math.pi * (self._step_count - 2 * T_max) / T_max))\n",
        "                    / 2\n",
        "                )\n",
        "            return lr_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjPeJQtPpPej"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpmxRhCHpPek"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\" Downsamples by a fac of 2 \"\"\"\n",
        "\n",
        "    def __init__(self, in_feat_dim, codebook_dim, hidden_dim=128, num_res_blocks=0):\n",
        "        super().__init__()\n",
        "        blocks = [\n",
        "            nn.Conv2d(in_feat_dim, hidden_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "            Mish(), # Activation function x * torch.tanh(F.softplus(x))\n",
        "            nn.Conv2d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
        "            Mish(), # Activation function x * torch.tanh(F.softplus(x))\n",
        "        ]\n",
        "\n",
        "        for _ in range(num_res_blocks):\n",
        "            blocks.append(ResBlock(hidden_dim, hidden_dim // 2))\n",
        "\n",
        "        blocks.append(nn.Conv2d(hidden_dim, codebook_dim, kernel_size=1))\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\" Upsamples by a fac of 2 \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, in_feat_dim, out_feat_dim, hidden_dim=128, num_res_blocks=0, very_bottom=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.very_bottom = very_bottom\n",
        "        self.out_feat_dim = out_feat_dim # num channels on bottom layer\n",
        "\n",
        "        blocks = [nn.Conv2d(in_feat_dim, hidden_dim, kernel_size=3, padding=1), Mish()]\n",
        "\n",
        "        for _ in range(num_res_blocks):\n",
        "            blocks.append(ResBlock(hidden_dim, hidden_dim // 2))\n",
        "\n",
        "        blocks.extend([\n",
        "                Upsample(),\n",
        "                nn.Conv2d(hidden_dim, hidden_dim // 2, kernel_size=3, padding=1),\n",
        "                Mish(),\n",
        "                nn.Conv2d(hidden_dim // 2, out_feat_dim, kernel_size=3, padding=1),\n",
        "        ])\n",
        "\n",
        "        if very_bottom is True:\n",
        "            blocks.append(nn.Sigmoid())       \n",
        "        \n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, scale_factor=2):\n",
        "        super().__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.interpolate(x, scale_factor=self.scale_factor)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, channel):\n",
        "        super().__init__()\n",
        "        self.conv_1 = nn.Conv2d(in_channel, channel, kernel_size=3, padding=1)\n",
        "        self.conv_2 = nn.Conv2d(channel, in_channel, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = self.conv_1(inp)\n",
        "        x = mish(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = x + inp\n",
        "        return mish(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zybgfcIOpPel"
      },
      "source": [
        "class VQCodebook(nn.Module):\n",
        "    def __init__(self, codebook_slots, codebook_dim, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.codebook_slots = codebook_slots\n",
        "        self.codebook_dim = codebook_dim\n",
        "        self.temperature = temperature\n",
        "        self.codebook = nn.Parameter(torch.randn(codebook_slots, codebook_dim))\n",
        "        self.log_slots_const = np.log(self.codebook_slots)\n",
        "\n",
        "    def z_e_to_z_q(self, z_e, soft=True):\n",
        "        bs, feat_dim, w, h = z_e.shape\n",
        "        assert feat_dim == self.codebook_dim\n",
        "        z_e = z_e.permute(0, 2, 3, 1).contiguous()\n",
        "        z_e_flat = z_e.view(bs * w * h, feat_dim)\n",
        "        codebook_sqr = torch.sum(self.codebook ** 2, dim=1)\n",
        "        z_e_flat_sqr = torch.sum(z_e_flat ** 2, dim=1, keepdim=True)\n",
        "\n",
        "        distances = torch.addmm(  # matrix multiplication\n",
        "            codebook_sqr + z_e_flat_sqr, z_e_flat, self.codebook.t(), alpha=-2.0, beta=1.0\n",
        "        )\n",
        "\n",
        "        if soft is True:\n",
        "            dist = RelaxedOneHotCategorical(self.temperature, logits=-distances)\n",
        "            soft_onehot = dist.rsample()\n",
        "            hard_indices = torch.argmax(soft_onehot, dim=1).view(bs, w, h)\n",
        "            z_q = (soft_onehot @ self.codebook).view(bs, w, h, feat_dim)\n",
        "            \n",
        "            # entropy loss\n",
        "            KL = dist.probs * (dist.probs.add(1e-9).log() + self.log_slots_const)\n",
        "            KL = KL.view(bs, w, h, self.codebook_slots).sum(dim=(1,2,3)).mean()\n",
        "            \n",
        "            # probability-weighted commitment loss    \n",
        "            commit_loss = (dist.probs.view(bs, w, h, self.codebook_slots) * distances.view(bs, w, h, self.codebook_slots)).sum(dim=(1,2,3)).mean()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                dist = Categorical(logits=-distances)\n",
        "                hard_indices = dist.sample().view(bs, w, h)\n",
        "                hard_onehot = (\n",
        "                    F.one_hot(hard_indices, num_classes=self.codebook_slots)\n",
        "                    .type_as(self.codebook)\n",
        "                    .view(bs * w * h, self.codebook_slots)\n",
        "                )\n",
        "                z_q = (hard_onehot @ self.codebook).view(bs, w, h, feat_dim)\n",
        "                \n",
        "                # entropy loss\n",
        "                KL = dist.probs * (dist.probs.add(1e-9).log() + np.log(self.codebook_slots))\n",
        "                KL = KL.view(bs, w, h, self.codebook_slots).sum(dim=(1,2,3)).mean()\n",
        "\n",
        "                commit_loss = 0.0\n",
        "\n",
        "        z_q = z_q.permute(0, 3, 1, 2)\n",
        "\n",
        "        return z_q, hard_indices, KL, commit_loss\n",
        "\n",
        "    def lookup(self, ids: torch.Tensor):\n",
        "        return F.embedding(ids, self.codebook).permute(0, 3, 1, 2)\n",
        "\n",
        "    def quantize(self, z_e, soft=False):\n",
        "        with torch.no_grad():\n",
        "            z_q, indices, _, _ = self.z_e_to_z_q(z_e, soft=soft)\n",
        "        return z_q, indices\n",
        "\n",
        "    def quantize_indices(self, z_e, soft=False):\n",
        "        with torch.no_grad():\n",
        "            _, indices, _, _ = self.z_e_to_z_q(z_e, soft=soft)\n",
        "        return indices\n",
        "\n",
        "    def forward(self, z_e):\n",
        "        z_q, indices, kl, commit_loss = self.z_e_to_z_q(z_e, soft=True)\n",
        "        return z_q, indices, kl, commit_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZRfiLPppPen"
      },
      "source": [
        "class GlobalNormalization(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    nn.Module to track and normalize input variables, calculates running estimates of data\n",
        "    statistics during training time.\n",
        "    Optional scale parameter to fix standard deviation of inputs to 1\n",
        "    Normalization atlassian page:\n",
        "    https://speechmatics.atlassian.net/wiki/spaces/INB/pages/905314814/Normalization+Module\n",
        "    Implementation details:\n",
        "    \"https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim, scale=False):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.register_buffer(\"running_ave\", torch.zeros(1, self.feature_dim, 1, 1))\n",
        "        self.register_buffer(\"total_frames_seen\", torch.Tensor([0]))\n",
        "        self.scale = scale\n",
        "        if self.scale:\n",
        "            self.register_buffer(\"running_sq_diff\", torch.zeros(1, self.feature_dim, 1, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        if self.training:\n",
        "            # Update running estimates of statistics\n",
        "            frames_in_input = inputs.shape[0] * inputs.shape[2] * inputs.shape[3]\n",
        "            updated_running_ave = (\n",
        "                self.running_ave * self.total_frames_seen + inputs.sum(dim=(0, 2, 3), keepdim=True)\n",
        "            ) / (self.total_frames_seen + frames_in_input)\n",
        "\n",
        "            if self.scale:\n",
        "                # Update the sum of the squared differences between inputs and mean\n",
        "                self.running_sq_diff = self.running_sq_diff + (\n",
        "                    (inputs - self.running_ave) * (inputs - updated_running_ave)\n",
        "                ).sum(dim=(0, 2, 3), keepdim=True)\n",
        "\n",
        "            self.running_ave = updated_running_ave\n",
        "            self.total_frames_seen = self.total_frames_seen + frames_in_input\n",
        "\n",
        "        if self.scale:\n",
        "            std = torch.sqrt(self.running_sq_diff / self.total_frames_seen)\n",
        "            inputs = (inputs - self.running_ave) / std\n",
        "        else:\n",
        "            inputs = inputs - self.running_ave\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def unnorm(self, inputs):\n",
        "        if self.scale:\n",
        "            std = torch.sqrt(self.running_sq_diff / self.total_frames_seen)\n",
        "            inputs = inputs*std + self.running_ave\n",
        "        else:\n",
        "            inputs = inputs + self.running_ave\n",
        "\n",
        "        return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "626nG3JCpPeo"
      },
      "source": [
        "class HQA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        prev_model,\n",
        "        input_feat_dim,\n",
        "        codebook_slots=512,\n",
        "        codebook_dim=64,\n",
        "        enc_hidden_dim=16,\n",
        "        dec_hidden_dim=32,\n",
        "        gs_temp=0.667,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.prev_model = prev_model\n",
        "        self.encoder = Encoder(input_feat_dim, codebook_dim, enc_hidden_dim)\n",
        "        self.codebook = VQCodebook(codebook_slots, codebook_dim, gs_temp)\n",
        "        self.decoder = Decoder(\n",
        "            codebook_dim,\n",
        "            input_feat_dim,\n",
        "            dec_hidden_dim,\n",
        "            very_bottom=prev_model is None,\n",
        "        )\n",
        "        self.normalize = GlobalNormalization(codebook_dim, scale=True)\n",
        "\n",
        "    def parameters(self, prefix=\"\", recurse=True):\n",
        "        for module in [self.encoder, self.codebook, self.decoder]:\n",
        "            for name, param in module.named_parameters(recurse=recurse):\n",
        "                yield param\n",
        "\n",
        "    @classmethod\n",
        "    def init_higher(cls, prev_model, **kwargs):\n",
        "        model = HQA(prev_model, prev_model.codebook.codebook_dim, **kwargs)\n",
        "        model.prev_model.eval()\n",
        "        return model\n",
        "    \n",
        "    @classmethod\n",
        "    def init_bottom(cls, input_feat_dim, **kwargs):\n",
        "        model = HQA(None, input_feat_dim, **kwargs)\n",
        "        return model\n",
        "        \n",
        "    def forward(self, img):\n",
        "        z_e_lower = self.encode_lower(img)\n",
        "        z_e = self.encoder(z_e_lower)\n",
        "        z_q, indices, kl, commit_loss = self.codebook(z_e)\n",
        "        z_e_lower_tilde = self.decoder(z_q)\n",
        "        return z_e_lower_tilde, z_e_lower, z_q, z_e, indices, kl, commit_loss\n",
        "   \n",
        "    def forward_full_stack(self, img):\n",
        "        z_e = self.encode(img)\n",
        "        z_q, indices, kl, commit_loss = self.codebook(z_e)\n",
        "        img_recon_dist = self.decode(z_q)\n",
        "        return img_recon_dist, img, z_q, z_e, indices, kl, commit_loss\n",
        "\n",
        "    def encode_lower(self, x):\n",
        "        if self.prev_model is None:\n",
        "            return x\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                z_e_lower = self.prev_model.encode(x)\n",
        "                z_e_lower = self.normalize(z_e_lower)\n",
        "            return z_e_lower\n",
        "\n",
        "    def encode(self, x):\n",
        "        with torch.no_grad():\n",
        "            z_e_lower = self.encode_lower(x)\n",
        "            z_e = self.encoder(z_e_lower)\n",
        "        return z_e\n",
        "        \n",
        "    def decode_lower(self, z_q_lower):\n",
        "        with torch.no_grad():\n",
        "            recon = self.prev_model.decode(z_q_lower)           \n",
        "        return recon\n",
        "\n",
        "    def decode(self, z_q):\n",
        "        with torch.no_grad():\n",
        "            if self.prev_model is not None:\n",
        "                z_e_u = self.normalize.unnorm(self.decoder(z_q))\n",
        "                z_q_lower_tilde = self.prev_model.quantize(z_e_u)\n",
        "                recon = self.decode_lower(z_q_lower_tilde)\n",
        "            else:\n",
        "                recon = self.decoder(z_q)\n",
        "        return recon\n",
        "\n",
        "    def quantize(self, z_e):\n",
        "        z_q, _ = self.codebook.quantize(z_e)\n",
        "        return z_q\n",
        "\n",
        "    def reconstruct_average(self, x, num_samples=10):\n",
        "        \"\"\"Average over stochastic edecodes\"\"\"\n",
        "        b, c, h, w = x.shape\n",
        "        result = torch.empty((num_samples, b, c, h, w)).to(device)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            result[i] = self.decode(self.quantize(self.encode(x)))\n",
        "        return result.mean(0)\n",
        "\n",
        "    def reconstruct(self, x):\n",
        "        return self.decode(self.quantize(self.encode(x)))\n",
        "    \n",
        "    def reconstruct_from_codes(self, codes):\n",
        "        return self.decode(self.codebook.lookup(codes))\n",
        "    \n",
        "    def reconstruct_from_z_e(self, z_e):\n",
        "        return self.decode(self.quantize(z_e))\n",
        "    \n",
        "    def recon_loss(self, orig, recon):\n",
        "        return F.mse_loss(orig, recon, reduction='none').sum(dim=(1,2,3)).mean()\n",
        "\n",
        "    def __len__(self):\n",
        "        i = 1\n",
        "        layer = self\n",
        "        while layer.prev_model is not None:\n",
        "            i += 1\n",
        "            layer = layer.prev_model\n",
        "        return i\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        max_layer = len(self) - 1\n",
        "        if idx > max_layer:\n",
        "            raise IndexError(\"layer does not exist\")\n",
        "\n",
        "        layer = self\n",
        "        for _ in range(max_layer - idx):\n",
        "            layer = layer.prev_model\n",
        "        return layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPVnEfmMpPep"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM1eQAPcpPep"
      },
      "source": [
        "def show_recon(img, *models):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=len(models), figsize=(10 * len(models), 5))\n",
        "\n",
        "    if not isinstance(axes, np.ndarray):\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        model.eval()\n",
        "    \n",
        "   \n",
        "        img_ = img.unsqueeze(0)\n",
        "        recon = model.reconstruct(img_).squeeze()\n",
        "\n",
        "        padding = np.ones([img.shape[0], 64, 1])\n",
        "        output = np.hstack([\n",
        "                            double_swap(img.cpu()),\n",
        "                            double_swap(padding), \n",
        "                            double_swap(recon.cpu()),\n",
        "                            double_swap(padding),\n",
        "                            double_swap(np.abs((img-recon).cpu())),\n",
        "                            ])\n",
        "        # V1\n",
        "        # output = np.moveaxis(output, 0, -1)\n",
        "        # V2\n",
        "        axes[i].imshow( output, vmin=0, vmax=1)\n",
        "\n",
        "\n",
        "\n",
        "def get_bit_usage(indices):\n",
        "    \"\"\" Calculate bits used by latent space vs max possible \"\"\"\n",
        "    num_latents = indices.shape[0] * indices.shape[1] * indices.shape[2]\n",
        "    avg_probs = F.one_hot(indices).float().mean(dim=(0, 1, 2, 3))\n",
        "    highest_prob = torch.max(avg_probs)\n",
        "    bits = (-(avg_probs * torch.log2(avg_probs + 1e-10)).sum()) * num_latents\n",
        "    max_bits = math.log2(256) * num_latents\n",
        "    return bits, max_bits, highest_prob\n",
        "\n",
        "\n",
        "def decay_temp_linear(step, total_steps, temp_base, temp_min=0.001):\n",
        "    factor = 1.0 - (step/total_steps)\n",
        "    return temp_min + (temp_base - temp_min) * factor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctGMkQKCpPeq"
      },
      "source": [
        "def get_loss_hqa(img, model, epoch, step, commit_threshold=0.6, log=None):\n",
        "    recon, orig, z_q, z_e, indices, KL, commit_loss = model(img)\n",
        "    recon_loss = model.recon_loss(orig, recon)\n",
        "    \n",
        "    # calculate loss\n",
        "    dims = np.prod(recon.shape[1:]) # orig_w * orig_h * num_channels\n",
        "    loss = recon_loss/dims + 0.001*KL/dims + 0.001*(commit_loss)/dims\n",
        "    \n",
        "    # logging    \n",
        "    if step % 20 == 0:\n",
        "        nll = recon_loss\n",
        "        elbo = -(nll + KL)  \n",
        "        distortion_bpd = nll / dims / np.log(2)\n",
        "        rate_bpd = KL / dims / np.log(2)\n",
        "        \n",
        "        bits, max_bits, highest_prob = get_bit_usage(indices)\n",
        "        bit_usage_frac = bits / max_bits\n",
        "        \n",
        "        time = datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "        log_line = f\"{time}, epoch={epoch}, step={step}, loss={loss:.5f}, distortion={distortion_bpd:.3f}, rate={rate_bpd:.3f}, -elbo={-elbo:.5f}, nll={nll:.5f}, KL={KL:.5f}, commit_loss={commit_loss:.5f}, bit_usage={bit_usage_frac:.5f}, highest_prob={highest_prob:.3f}, temp={model.codebook.temperature:.5f}\"\n",
        "        print(log_line)\n",
        "\n",
        "        if log is not None:\n",
        "            with open(log, \"a\") as logfile:\n",
        "                logfile.write(log_line + \"\\n\")\n",
        "                \n",
        "    return loss, indices\n",
        "\n",
        "\n",
        "def train(model, optimizer, scheduler, epochs, decay=True, log=None):\n",
        "    step = 0\n",
        "    model.train()\n",
        "    temp_base = model.codebook.temperature\n",
        "    code_count = torch.zeros(model.codebook.codebook_slots).to(device)\n",
        "    total_steps = len(dl_train)*epochs\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for x in dl_train:\n",
        "            x = x.to(device)\n",
        "            \n",
        "            # anneal temperature\n",
        "            if decay is True:\n",
        "                model.codebook.temperature = decay_temp_linear(step+1, total_steps, temp_base, temp_min=0.001) \n",
        "            \n",
        "            loss, indices = get_loss_hqa(x, model, epoch, step, log=log)\n",
        "                \n",
        "            # take training step    \n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()     \n",
        "                \n",
        "            # code reset every 20 steps\n",
        "            indices_onehot = F.one_hot(indices, num_classes=model.codebook.codebook_slots).float()\n",
        "            code_count = code_count + indices_onehot.sum(dim=(0, 1, 2, 3))\n",
        "            if step % 20 == 0:\n",
        "                with torch.no_grad():\n",
        "                    max_count, most_used_code = torch.max(code_count, dim=0)\n",
        "                    frac_usage = code_count / max_count\n",
        "                    z_q_most_used = model.codebook.lookup(most_used_code.view(1, 1, 1)).squeeze()\n",
        "\n",
        "                    min_frac_usage, min_used_code = torch.min(frac_usage, dim=0)\n",
        "                    if min_frac_usage < 0.03:\n",
        "                        print(f'reset code {min_used_code}')\n",
        "                        moved_code = z_q_most_used + torch.randn_like(z_q_most_used) / 100\n",
        "                        model.codebook.codebook[min_used_code] = moved_code\n",
        "                    code_count = torch.zeros_like(code_count)\n",
        "\n",
        "            step += 1\n",
        "        if epoch % 50 == 0:\n",
        "            for n in range(0, 3):\n",
        "                reshaped = test_x[n].reshape((3,64,64))\n",
        "                show_recon(reshaped, model)\n",
        "                plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBNI1zmmpPer"
      },
      "source": [
        "def train_full_stack(root, exp_name, epochs=150, lr=4e-4):\n",
        "    \n",
        "    enc_hidden_sizes = [64, 64, 512, 512, 512, 512, 512]\n",
        "    dec_hidden_sizes = [64, 64, 512, 512, 512, 512, 512]\n",
        "    \n",
        "    os.makedirs(root + \"/log\", exist_ok=True)\n",
        "    \n",
        "    for i in range(len(enc_hidden_sizes)):\n",
        "        print(f\"training layer{i}\")\n",
        "        if i == 0:\n",
        "            hqa = HQA.init_bottom(\n",
        "                input_feat_dim=3, # over here the number of initial channels (Grayscale = 1 , RGB = 3)\n",
        "                enc_hidden_dim=enc_hidden_sizes[i],\n",
        "                dec_hidden_dim=dec_hidden_sizes[i],\n",
        "            ).to(device)\n",
        "        else:\n",
        "            hqa = HQA.init_higher(\n",
        "                hqa_prev,\n",
        "                enc_hidden_dim=enc_hidden_sizes[i],\n",
        "                dec_hidden_dim=dec_hidden_sizes[i],\n",
        "            ).to(device)\n",
        "        \n",
        "        print(f\"layer{i} param count {sum(x.numel() for x in hqa.parameters()):,}\")\n",
        "        \n",
        "        log_file = f\"{root}/log/{exp_name}_l{i}.log\"\n",
        "        opt = RAdam(hqa.parameters(), lr=lr)\n",
        "        scheduler = FlatCA(opt, steps=epochs*len(dl_train), eta_min=lr/10)\n",
        "        train(hqa, opt, scheduler, epochs, log=log_file)\n",
        "        hqa_prev = hqa\n",
        "    \n",
        "    torch.save(hqa, f\"{root}/{exp_name}.pt\")\n",
        "    \n",
        "    return hqa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yABvIzEYpPes"
      },
      "source": [
        "### Train HQA Stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kVO-BGlpPes",
        "scrolled": true
      },
      "source": [
        "# Train a HQA stack\n",
        "model_name = \"hqa_model\"\n",
        "models_dir = f\"{os.getcwd()}/models\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "if not os.path.isfile(f\"{models_dir}/{model_name}.pt\"):\n",
        "  hqa_model = train_full_stack(models_dir, model_name, epochs=150)\n",
        "else:\n",
        "  hqa_model = torch.load(f\"{models_dir}/{model_name}.pt\")\n",
        "\n",
        "hqa_model.eval()\n",
        "    \n",
        "layer_names = [\"Layer 0\", \"Layer 1\", \"Layer 2\", \"Layer 3\", \"Layer 4\", \"Layer 5\", \"Layer 6 Final\"]\n",
        "layer_descriptions = [\n",
        "    \"downsample 1 in each dimension, latent space size of 64x64\",\n",
        "    \"downsample 2 in each dimension, latent space size of 32x32\",\n",
        "    \"downsample 4 in each dimension, latent space size of 16x16\",\n",
        "    \"downsample 8 in each dimension, latent space size of 8x8\",\n",
        "    \"downsample 16 in each dimension, latent space size of 4x4\",\n",
        "    \"downsample 32 in each dimension, latent space size of 2x2\",\n",
        "    \"downsample 64 in each dimension, latent space size of 1x1\",\n",
        "]\n",
        "torch.save(hqa_model.state_dict(), f\"{models_dir}/{model_name}.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOh5BIzOpPex"
      },
      "source": [
        "### Layer Reconstructions\n",
        "Final layer recons match HQA recons from Table 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA73yED1pPey"
      },
      "source": [
        "def double_swap(img):\n",
        "  img = np.swapaxes(img, 0,-1)\n",
        "  img = np.swapaxes(img, 0,1)\n",
        "  return img\n",
        "\n",
        "def show_image(im_data, scale=10):\n",
        "    dpi = matplotlib.rcParams['figure.dpi']\n",
        "    width, height ,channel = im_data.shape\n",
        "    figsize = scale * width / float(dpi), scale * height / float(dpi)\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = fig.add_axes([0, 0, 1, 1])\n",
        "    # Hide spines, ticks, etc.\n",
        "    ax.axis('off')\n",
        "    ax.imshow(im_data, vmin=0, vmax=1)\n",
        "    plt.show();\n",
        "    ax.set(xlim=[0, width], ylim=[height, 0], aspect=1)\n",
        "def recon_comparison(model, names, descriptions, indexes=[0, 4, 15, 16, 18]):\n",
        "    images = []\n",
        "    for idx in indexes:\n",
        "        image = celeba_dataset[idx]    \n",
        "        img = image.to(device).squeeze()\n",
        "        img = double_swap(img.cpu().numpy())\n",
        "        images.append(img)\n",
        "    print(\"Original images to be reconstructed\")\n",
        "    output = np.hstack(images)\n",
        "\n",
        "    show_image(output,2)\n",
        "    \n",
        "    for layer, name, description in zip(model, names, descriptions):\n",
        "        images = []\n",
        "        \n",
        "        for idx in indexes:\n",
        "            image = celeba_dataset[idx]    \n",
        "            img = image.to(device).squeeze()\n",
        "            \n",
        "            for_recon = img.unsqueeze(0)\n",
        "            layer.eval()\n",
        "            recon = layer.reconstruct(for_recon).squeeze()\n",
        "            recon = double_swap(recon.cpu().numpy())\n",
        "            images.append(recon)\n",
        "        \n",
        "        print(f\"{name}: {description}\")\n",
        "        output = np.hstack(images)\n",
        "        show_image(output,2)\n",
        "# Show reconstruction comparison over each layer in HQA\n",
        "recon_comparison(hqa_model, layer_names, layer_descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg74Bli_pPez"
      },
      "source": [
        "### Distortions\n",
        "HQA distortions in Figure 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO1FllDzpPez"
      },
      "source": [
        "def get_rate_upper_bound(model, example_input):\n",
        "    assert len(example_input.shape) == 4, \"Expected (1, num_channels, x_h, x_w)\"\n",
        "    assert example_input.shape[0] == 1, \"Please provide example with batch_size=1\"\n",
        "    \n",
        "    z_e = model.encode(example_input)\n",
        "    _, top_indices, _, _ = model.codebook(z_e)\n",
        "        \n",
        "    # assume worst case scenario: we have a uniform usage of all our codes\n",
        "    rate_bound = top_indices[0].numel() * np.log2(model.codebook.codebook_slots)\n",
        "\n",
        "    return rate_bound\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    total_nll = []\n",
        "    total_kl = []\n",
        "    \n",
        "    for x in dl_train:\n",
        "        img = x.to(device)       \n",
        "        recon, orig, z_q, z_e, indices, kl, _ = model.forward_full_stack(img)       \n",
        "        recon_loss = model[0].recon_loss(img, recon)        \n",
        "        total_nll.append(recon_loss.item())\n",
        "        if kl != 0:\n",
        "            total_kl.append(kl.item())\n",
        "        else:\n",
        "            total_kl.append(kl)\n",
        "    \n",
        "    dims = np.prod(x.shape[1:])\n",
        "    kl_mean = np.mean(total_kl)\n",
        "    nll_mean = np.mean(total_nll)\n",
        "    distortion_bpd = nll_mean / dims / np.log(2)\n",
        "    rate_bpd = kl_mean / dims / np.log(2)\n",
        "    elbo = -(nll_mean + kl_mean)\n",
        "    \n",
        "    rate_bound = get_rate_upper_bound(model, img[0].unsqueeze(0))\n",
        "    \n",
        "    return distortion_bpd, rate_bound\n",
        "\n",
        "\n",
        "def get_rd_data(model):\n",
        "    dist = []\n",
        "    rates = []\n",
        "    \n",
        "    for i, _ in enumerate(model):\n",
        "        d, r = test(model[i])\n",
        "        dist.append(float(d))\n",
        "        rates.append(float(r))\n",
        "    \n",
        "    return dist, rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LmW-8bspPe0"
      },
      "source": [
        "# Layer distortions\n",
        "distortions, rates = get_rd_data(hqa_model)\n",
        "print(\"Name \\t\\t Distortion \\t Rate\")\n",
        "for dist, rate, name in zip(distortions, rates, layer_names):\n",
        "    print(f\"{name} \\t {dist:.4f} \\t {int(rate)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH3w5jLopPe1"
      },
      "source": [
        "### \"Free\" Samples: enumerating over all 1x1 latents\n",
        "Appendix Figure 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-4Z2tq3pPe1"
      },
      "source": [
        "num_codes = hqa_model.codebook.codebook_slots\n",
        "results = torch.Tensor(num_codes, 3, 32, 32).to(device)\n",
        "count=0\n",
        "for i in range(num_codes):\n",
        "    codes = torch.LongTensor([i]).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    results[count] = hqa_model.reconstruct_from_codes(codes)\n",
        "    count += 1\n",
        "        \n",
        "grid_img = make_grid(results.cpu(), nrow=16)\n",
        "grid_img = double_swap(grid_img)\n",
        "show_image(grid_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N010LoM4pPe2"
      },
      "source": [
        "### Final Layer Interpolations\n",
        "Appendix Figure 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_KmpQFNpPe2"
      },
      "source": [
        "grid_x = grid_y = 16\n",
        "results = torch.Tensor(grid_x * grid_y, 3,64, 64)\n",
        "i = 0\n",
        "\n",
        "for j in range(grid_y):\n",
        "    x_a = celeba_dataset[j]\n",
        "    x_b= celeba_dataset[j+grid_y]\n",
        "    point_1 = hqa_model.encode(x_a.unsqueeze(0).to(device)).cpu()\n",
        "    point_2 = hqa_model.encode(x_b.unsqueeze(0).to(device)).cpu()\n",
        "    interpolate_x = np.linspace(point_1[0], point_2[0], grid_x)\n",
        "\n",
        "    for z_e_interpolated in interpolate_x:\n",
        "        z_e_i = torch.Tensor(z_e_interpolated).unsqueeze(0).to(device)\n",
        "        z_q = hqa_model.quantize(z_e_i)\n",
        "        results[i] = hqa_model.decode(z_q).squeeze()\n",
        "        i += 1\n",
        "            \n",
        "grid_img = make_grid(results.cpu(), nrow=grid_x)\n",
        "grid_img = double_swap(grid_img)\n",
        "\n",
        "show_image(grid_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp7tjfFSpPe3"
      },
      "source": [
        "### Stochastic Reconstructions\n",
        "Appendix Figure 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_lIBJGvpPe3"
      },
      "source": [
        "# Show held-out reconstructions: [ORIG, 14xSAMPLE, AVERAGED_10_SAMPLES]\n",
        "grid_x = grid_y = 16\n",
        "results = torch.Tensor(grid_x * grid_y,3,64, 64)\n",
        "\n",
        "result_idx = 0\n",
        "for test_idx in range(grid_y):\n",
        "    x_a = celeba_dataset[test_idx]\n",
        "    img = x_a.squeeze().to(device)\n",
        "    img_ = img.unsqueeze(0)\n",
        "    num_examples = 5\n",
        "    \n",
        "    # ORIG\n",
        "    results[result_idx] = img\n",
        "    result_idx += 1\n",
        "    \n",
        "    # 14 RANDOM STOCHASTIC DECODES\n",
        "    for _ in range(grid_x -2):\n",
        "        results[result_idx] = hqa_model.reconstruct(img_)\n",
        "        result_idx += 1\n",
        "    \n",
        "    # AVERAGED SAMPLES\n",
        "    results[result_idx] = hqa_model.reconstruct_average(img_, num_samples=14).squeeze()\n",
        "    result_idx += 1\n",
        "\n",
        "grid_img = make_grid(results.cpu(), nrow=grid_x)\n",
        "grid_img = double_swap(grid_img)\n",
        "\n",
        "show_image(grid_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdHWbL7spPe4"
      },
      "source": [
        "### Layer-wise Interpolations\n",
        "Appendix Table 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWqqJYafpPe4"
      },
      "source": [
        "def interpolate(a, b, vqvae, grid_x=16):\n",
        "    images = []\n",
        "    \n",
        "    x_a = celeba_dataset[a]\n",
        "    x_b = celeba_dataset[b]\n",
        "    point_1 = vqvae.encode(x_a.unsqueeze(0).to(device))\n",
        "    point_2 = vqvae.encode(x_b.unsqueeze(0).to(device))\n",
        "\n",
        "    interpolate_x = np.linspace(point_1[0].cpu().numpy(), point_2[0].cpu().numpy(), grid_x)\n",
        "    \n",
        "    results = torch.Tensor(len(interpolate_x), 3,64, 64)\n",
        "    for i, z_e_interpolated in enumerate(interpolate_x):       \n",
        "        z_e = torch.Tensor(z_e_interpolated).unsqueeze(0).to(device)\n",
        "        z_q = vqvae.quantize(z_e)\n",
        "        recon = vqvae.decode(z_q).squeeze() \n",
        "        results[i] = recon\n",
        "\n",
        "    grid_img = make_grid(results.cpu(), nrow=grid_x)\n",
        "    grid_img = double_swap(grid_img)\n",
        "    show_image(grid_img)\n",
        "\n",
        "def show_original(idx):\n",
        "    x= celeba_dataset[idx]\n",
        "    image = x.squeeze()\n",
        "    image = double_swap(image)\n",
        "    show_image(image,2)\n",
        "    \n",
        "print(\"Originals\")\n",
        "show_original(1)\n",
        "show_original(9)\n",
        "for layer, name, description in zip(hqa_model, layer_names, layer_descriptions):\n",
        "    print(f\"{name} : {description}\")\n",
        "    interpolate(1, 9, layer, grid_x=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}