{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HQA_CelebA_V_Moez_'little data'(18_04_2021).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moez-RT/HQA/blob/main/HQA_CelebA_V_Moez_'little_data'(18_04_2021).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8lCFMMQotuc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk_aUuh4pPeD"
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import types\n",
        "import datetime\n",
        "import random\n",
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, distributions\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.autograd import Function\n",
        "from torch.distributions import RelaxedOneHotCategorical, Normal, Categorical\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CelebA \n",
        "from torchvision.utils import make_grid"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY7HXNXppPed",
        "outputId": "a393ccb3-bf1d-48b0-9d80-b523354b77dd"
      },
      "source": [
        "def set_seeds(seed=42, fully_deterministic=False):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "    if fully_deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seeds()\n",
        "\n",
        "print(f\"CUDA={torch.cuda.is_available()}\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA=True None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkXkrSZFpPee"
      },
      "source": [
        "### CelebA Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNSfXAlHZ0zl",
        "outputId": "dade0dc9-7265-46ea-ad98-0b616c4b8df9"
      },
      "source": [
        "!rm -rf data\n",
        "!rm -rf models\n",
        "import os\n",
        "import zipfile \n",
        "import gdown\n",
        "import torch\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "data_root = 'data/celeba'\n",
        "# Path to folder with the dataset\n",
        "dataset_folder = f'{data_root}/img_align_celeba'\n",
        "# URL for the CelebA dataset\n",
        "url = 'https://drive.google.com/uc?id=1cNIac61PSA_LqDFYFUeyaQYekYPc75NH'\n",
        "# Path to download the dataset to\n",
        "download_path = f'{data_root}/img_align_celeba.zip'\n",
        "\n",
        "# Create required directories \n",
        "if not os.path.exists(data_root):\n",
        "  os.makedirs(data_root)\n",
        "  os.makedirs(dataset_folder)\n",
        "\n",
        "# Download the dataset from google drive\n",
        "gdown.download(url, download_path, quiet=False)\n",
        "\n",
        "# Unzip the downloaded file \n",
        "with zipfile.ZipFile(download_path, 'r') as ziphandler:\n",
        "  ziphandler.extractall(dataset_folder)\n",
        "\n",
        "## Create a custom Dataset class\n",
        "class CelebADataset(Dataset):\n",
        "  def __init__(self, root_dir, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      root_dir (string): Directory with all the images\n",
        "      transform (callable, optional): transform to be applied to each image sample\n",
        "    \"\"\"\n",
        "    # Read names of images in the root directory\n",
        "    image_names = os.listdir(root_dir)\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform \n",
        "    self.image_names = natsorted(image_names)\n",
        "\n",
        "  def __len__(self): \n",
        "    return len(self.image_names)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get the path to the image \n",
        "    img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
        "    # Load image and convert it to RGB\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    # Apply transformations to the image\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cNIac61PSA_LqDFYFUeyaQYekYPc75NH\n",
            "To: /content/data/celeba/img_align_celeba.zip\n",
            "1.44GB [00:11, 124MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irCXkE_n2Yb-"
      },
      "source": [
        "\n",
        "## Load the dataset \n",
        "# Path to directory with all the images\n",
        "img_folder = f'{dataset_folder}/img_align_celeba'\n",
        "# Spatial size of training images, images are resized to this size.\n",
        "image_size = 64\n",
        "# Transformations to be applied to each individual image sample\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "\n",
        "  \n",
        "])\n",
        "# Load the dataset from file and apply transformations\n",
        "celeba_dataset = CelebADataset(img_folder, transform)\n",
        "\n",
        "## Create a dataloader \n",
        "# Batch size during training\n",
        "batch_size = 256\n",
        "# Number of workers for the dataloader\n",
        "num_workers = 0 if device.type == 'cuda' else 2\n",
        "# Whether to put fetched data tensors to pinned memory\n",
        "pin_memory = True if device.type == 'cuda' else False"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHJgTowBaAIY",
        "outputId": "b11cc800-a04b-4d69-dd08-5f7bfc3cb6d9"
      },
      "source": [
        "\n",
        "len_dataset = int(5e4)\n",
        "\n",
        "part_tr = torch.utils.data.random_split(celeba_dataset, [len_dataset, len(celeba_dataset) - len_dataset], generator=torch.Generator().manual_seed(42))[0]\n",
        "len(part_tr)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCB-YAEFpPef"
      },
      "source": [
        "\n",
        "dl_train = torch.utils.data.DataLoader(part_tr,\n",
        "                                                batch_size=batch_size,\n",
        "                                                num_workers=num_workers,\n",
        "                                                pin_memory=pin_memory,\n",
        "                                                shuffle=True)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YNRMQQIz-o7"
      },
      "source": [
        "test_x = next(iter(dl_train))\n",
        "test_x = test_x.to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSDTGLHsFIaq",
        "outputId": "630908bc-b6de-4ed9-f7e6-257fa917cf0a"
      },
      "source": [
        "z = None\n",
        "for x  in dl_train:\n",
        "  z = x[0]\n",
        "  print(x[0].shape)\n",
        "  print(len(x))\n",
        "  break\n",
        "y= z.permute(1,2,0).contiguous()\n",
        "print(y.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 64, 64])\n",
            "256\n",
            "torch.Size([64, 64, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qsQnvoZ-sKy",
        "outputId": "59f44f45-3404-4195-ac7b-138045769baa"
      },
      "source": [
        "z[:].shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "C9MXjj5at-2q",
        "outputId": "b0d1e298-8744-4dc5-89d0-b45243f19e7a"
      },
      "source": [
        "plt.imshow(y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7445e119d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1mRs_GQpPeg"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxVZmDd5pPeg"
      },
      "source": [
        "\n",
        "def double_swap(img):\n",
        "  img = np.swapaxes(img, 0,-1)\n",
        "  img = np.swapaxes(img, 0,1)\n",
        "  return img\n",
        "\n",
        "def show_image(im_data, scale=10):\n",
        "    dpi = matplotlib.rcParams['figure.dpi']\n",
        "    width, height ,channel = im_data.shape\n",
        "    figsize = scale * width / float(dpi), scale * height / float(dpi)\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = fig.add_axes([0, 0, 1, 1])\n",
        "    # Hide spines, ticks, etc.\n",
        "    ax.axis('off')\n",
        "    ax.imshow(im_data, vmin=0, vmax=1)\n",
        "    plt.show();\n",
        "    ax.set(xlim=[0, width], ylim=[height, 0], aspect=1)\n",
        "\n",
        "\n",
        "class RAdam(Optimizer):\n",
        "    def __init__(\n",
        "        self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True\n",
        "    ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "\n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if \"betas\" in param and (\n",
        "                    param[\"betas\"][0] != betas[0] or param[\"betas\"][1] != betas[1]\n",
        "                ):\n",
        "                    param[\"buffer\"] = [[None, None, None] for _ in range(10)]\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            betas=betas,\n",
        "            eps=eps,\n",
        "            weight_decay=weight_decay,\n",
        "            buffer=[[None, None, None] for _ in range(10)],\n",
        "        )\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                buffered = group[\"buffer\"][int(state[\"step\"] % 10)]\n",
        "                if state[\"step\"] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state[\"step\"]\n",
        "                    beta2_t = beta2 ** state[\"step\"]\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = math.sqrt(\n",
        "                            (1 - beta2_t)\n",
        "                            * (N_sma - 4)\n",
        "                            / (N_sma_max - 4)\n",
        "                            * (N_sma - 2)\n",
        "                            / N_sma\n",
        "                            * N_sma_max\n",
        "                            / (N_sma_max - 2)\n",
        "                        ) / (1 - beta1 ** state[\"step\"])\n",
        "                    elif self.degenerated_to_sgd:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n",
        "                    else:\n",
        "                        step_size = -1\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group[\"weight_decay\"] != 0:\n",
        "                        p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group[\"lr\"], exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif step_size > 0:\n",
        "                    if group[\"weight_decay\"] != 0:\n",
        "                        p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n",
        "                    p_data_fp32.add_(-step_size * group[\"lr\"], exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Mish - \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\"\n",
        "# https://arxiv.org/abs/1908.08681v1\n",
        "# implemented for PyTorch / FastAI by lessw2020\n",
        "# github: https://github.com/lessw2020/mish\n",
        "def mish(x):\n",
        "    return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class Mish(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return mish(x)\n",
        "    \n",
        "# https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html\n",
        "class FlatCA(_LRScheduler):\n",
        "    def __init__(self, optimizer, steps, eta_min=0, last_epoch=-1):\n",
        "        self.steps = steps\n",
        "        self.eta_min = eta_min\n",
        "        super(FlatCA, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr_list = []\n",
        "        T_max = self.steps / 3\n",
        "        for base_lr in self.base_lrs:\n",
        "            # flat if first 2/3\n",
        "            if 0 <= self._step_count < 2 * T_max:\n",
        "                lr_list.append(base_lr)\n",
        "            # annealed if last 1/3\n",
        "            else:\n",
        "                lr_list.append(\n",
        "                    self.eta_min\n",
        "                    + (base_lr - self.eta_min)\n",
        "                    * (1 + math.cos(math.pi * (self._step_count - 2 * T_max) / T_max))\n",
        "                    / 2\n",
        "                )\n",
        "            return lr_list"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjPeJQtPpPej"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpmxRhCHpPek"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\" Downsamples by a fac of 2 \"\"\"\n",
        "\n",
        "    def __init__(self, in_feat_dim, codebook_dim, hidden_dim=128, num_res_blocks=0):\n",
        "        super().__init__()\n",
        "        blocks = [\n",
        "            nn.Conv2d(in_feat_dim, hidden_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "            Mish(), # Activation function x * torch.tanh(F.softplus(x))\n",
        "            nn.Conv2d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
        "            Mish(), # Activation function x * torch.tanh(F.softplus(x))\n",
        "        ]\n",
        "\n",
        "        for _ in range(num_res_blocks):\n",
        "            blocks.append(ResBlock(hidden_dim, hidden_dim // 2))\n",
        "\n",
        "        blocks.append(nn.Conv2d(hidden_dim, codebook_dim, kernel_size=1))\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\" Upsamples by a fac of 2 \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, in_feat_dim, out_feat_dim, hidden_dim=128, num_res_blocks=0, very_bottom=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.very_bottom = very_bottom\n",
        "        self.out_feat_dim = out_feat_dim # num channels on bottom layer\n",
        "\n",
        "        blocks = [nn.Conv2d(in_feat_dim, hidden_dim, kernel_size=3, padding=1), Mish()]\n",
        "\n",
        "        for _ in range(num_res_blocks):\n",
        "            blocks.append(ResBlock(hidden_dim, hidden_dim // 2))\n",
        "\n",
        "        blocks.extend([\n",
        "                Upsample(),\n",
        "                nn.Conv2d(hidden_dim, hidden_dim // 2, kernel_size=3, padding=1),\n",
        "                Mish(),\n",
        "                nn.Conv2d(hidden_dim // 2, out_feat_dim, kernel_size=3, padding=1),\n",
        "        ])\n",
        "\n",
        "        if very_bottom is True:\n",
        "            blocks.append(nn.Sigmoid())       \n",
        "        \n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, scale_factor=2):\n",
        "        super().__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.interpolate(x, scale_factor=self.scale_factor)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, channel):\n",
        "        super().__init__()\n",
        "        self.conv_1 = nn.Conv2d(in_channel, channel, kernel_size=3, padding=1)\n",
        "        self.conv_2 = nn.Conv2d(channel, in_channel, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = self.conv_1(inp)\n",
        "        x = mish(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = x + inp\n",
        "        return mish(x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zybgfcIOpPel"
      },
      "source": [
        "class VQCodebook(nn.Module):\n",
        "    def __init__(self, codebook_slots, codebook_dim, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.codebook_slots = codebook_slots\n",
        "        self.codebook_dim = codebook_dim\n",
        "        self.temperature = temperature\n",
        "        self.codebook = nn.Parameter(torch.randn(codebook_slots, codebook_dim))\n",
        "        self.log_slots_const = np.log(self.codebook_slots)\n",
        "\n",
        "    def z_e_to_z_q(self, z_e, soft=True):\n",
        "        bs, feat_dim, w, h = z_e.shape\n",
        "        assert feat_dim == self.codebook_dim\n",
        "        z_e = z_e.permute(0, 2, 3, 1).contiguous()\n",
        "        z_e_flat = z_e.view(bs * w * h, feat_dim)\n",
        "        codebook_sqr = torch.sum(self.codebook ** 2, dim=1)\n",
        "        z_e_flat_sqr = torch.sum(z_e_flat ** 2, dim=1, keepdim=True)\n",
        "\n",
        "        distances = torch.addmm(  # matrix multiplication\n",
        "            codebook_sqr + z_e_flat_sqr, z_e_flat, self.codebook.t(), alpha=-2.0, beta=1.0\n",
        "        )\n",
        "\n",
        "        if soft is True:\n",
        "            dist = RelaxedOneHotCategorical(self.temperature, logits=-distances)\n",
        "            soft_onehot = dist.rsample()\n",
        "            hard_indices = torch.argmax(soft_onehot, dim=1).view(bs, w, h)\n",
        "            z_q = (soft_onehot @ self.codebook).view(bs, w, h, feat_dim)\n",
        "            \n",
        "            # entropy loss\n",
        "            KL = dist.probs * (dist.probs.add(1e-9).log() + self.log_slots_const)\n",
        "            KL = KL.view(bs, w, h, self.codebook_slots).sum(dim=(1,2,3)).mean()\n",
        "            \n",
        "            # probability-weighted commitment loss    \n",
        "            commit_loss = (dist.probs.view(bs, w, h, self.codebook_slots) * distances.view(bs, w, h, self.codebook_slots)).sum(dim=(1,2,3)).mean()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                dist = Categorical(logits=-distances)\n",
        "                hard_indices = dist.sample().view(bs, w, h)\n",
        "                hard_onehot = (\n",
        "                    F.one_hot(hard_indices, num_classes=self.codebook_slots)\n",
        "                    .type_as(self.codebook)\n",
        "                    .view(bs * w * h, self.codebook_slots)\n",
        "                )\n",
        "                z_q = (hard_onehot @ self.codebook).view(bs, w, h, feat_dim)\n",
        "                \n",
        "                # entropy loss\n",
        "                KL = dist.probs * (dist.probs.add(1e-9).log() + np.log(self.codebook_slots))\n",
        "                KL = KL.view(bs, w, h, self.codebook_slots).sum(dim=(1,2,3)).mean()\n",
        "\n",
        "                commit_loss = 0.0\n",
        "\n",
        "        z_q = z_q.permute(0, 3, 1, 2)\n",
        "\n",
        "        return z_q, hard_indices, KL, commit_loss\n",
        "\n",
        "    def lookup(self, ids: torch.Tensor):\n",
        "        return F.embedding(ids, self.codebook).permute(0, 3, 1, 2)\n",
        "\n",
        "    def quantize(self, z_e, soft=False):\n",
        "        with torch.no_grad():\n",
        "            z_q, indices, _, _ = self.z_e_to_z_q(z_e, soft=soft)\n",
        "        return z_q, indices\n",
        "\n",
        "    def quantize_indices(self, z_e, soft=False):\n",
        "        with torch.no_grad():\n",
        "            _, indices, _, _ = self.z_e_to_z_q(z_e, soft=soft)\n",
        "        return indices\n",
        "\n",
        "    def forward(self, z_e):\n",
        "        z_q, indices, kl, commit_loss = self.z_e_to_z_q(z_e, soft=True)\n",
        "        return z_q, indices, kl, commit_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZRfiLPppPen"
      },
      "source": [
        "class GlobalNormalization(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    nn.Module to track and normalize input variables, calculates running estimates of data\n",
        "    statistics during training time.\n",
        "    Optional scale parameter to fix standard deviation of inputs to 1\n",
        "    Normalization atlassian page:\n",
        "    https://speechmatics.atlassian.net/wiki/spaces/INB/pages/905314814/Normalization+Module\n",
        "    Implementation details:\n",
        "    \"https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim, scale=False):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.register_buffer(\"running_ave\", torch.zeros(1, self.feature_dim, 1, 1))\n",
        "        self.register_buffer(\"total_frames_seen\", torch.Tensor([0]))\n",
        "        self.scale = scale\n",
        "        if self.scale:\n",
        "            self.register_buffer(\"running_sq_diff\", torch.zeros(1, self.feature_dim, 1, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        if self.training:\n",
        "            # Update running estimates of statistics\n",
        "            frames_in_input = inputs.shape[0] * inputs.shape[2] * inputs.shape[3]\n",
        "            updated_running_ave = (\n",
        "                self.running_ave * self.total_frames_seen + inputs.sum(dim=(0, 2, 3), keepdim=True)\n",
        "            ) / (self.total_frames_seen + frames_in_input)\n",
        "\n",
        "            if self.scale:\n",
        "                # Update the sum of the squared differences between inputs and mean\n",
        "                self.running_sq_diff = self.running_sq_diff + (\n",
        "                    (inputs - self.running_ave) * (inputs - updated_running_ave)\n",
        "                ).sum(dim=(0, 2, 3), keepdim=True)\n",
        "\n",
        "            self.running_ave = updated_running_ave\n",
        "            self.total_frames_seen = self.total_frames_seen + frames_in_input\n",
        "\n",
        "        if self.scale:\n",
        "            std = torch.sqrt(self.running_sq_diff / self.total_frames_seen)\n",
        "            inputs = (inputs - self.running_ave) / std\n",
        "        else:\n",
        "            inputs = inputs - self.running_ave\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def unnorm(self, inputs):\n",
        "        if self.scale:\n",
        "            std = torch.sqrt(self.running_sq_diff / self.total_frames_seen)\n",
        "            inputs = inputs*std + self.running_ave\n",
        "        else:\n",
        "            inputs = inputs + self.running_ave\n",
        "\n",
        "        return inputs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "626nG3JCpPeo"
      },
      "source": [
        "class HQA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        prev_model,\n",
        "        input_feat_dim,\n",
        "        codebook_slots=256,\n",
        "        codebook_dim=64,\n",
        "        enc_hidden_dim=16,\n",
        "        dec_hidden_dim=32,\n",
        "        gs_temp=0.667,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.prev_model = prev_model\n",
        "        self.encoder = Encoder(input_feat_dim, codebook_dim, enc_hidden_dim)\n",
        "        self.codebook = VQCodebook(codebook_slots, codebook_dim, gs_temp)\n",
        "        self.decoder = Decoder(\n",
        "            codebook_dim,\n",
        "            input_feat_dim,\n",
        "            dec_hidden_dim,\n",
        "            very_bottom=prev_model is None,\n",
        "        )\n",
        "        self.normalize = GlobalNormalization(codebook_dim, scale=True)\n",
        "\n",
        "    def parameters(self, prefix=\"\", recurse=True):\n",
        "        for module in [self.encoder, self.codebook, self.decoder]:\n",
        "            for name, param in module.named_parameters(recurse=recurse):\n",
        "                yield param\n",
        "\n",
        "    @classmethod\n",
        "    def init_higher(cls, prev_model, **kwargs):\n",
        "        model = HQA(prev_model, prev_model.codebook.codebook_dim, **kwargs)\n",
        "        model.prev_model.eval()\n",
        "        return model\n",
        "    \n",
        "    @classmethod\n",
        "    def init_bottom(cls, input_feat_dim, **kwargs):\n",
        "        model = HQA(None, input_feat_dim, **kwargs)\n",
        "        return model\n",
        "        \n",
        "    def forward(self, img):\n",
        "        z_e_lower = self.encode_lower(img)\n",
        "        z_e = self.encoder(z_e_lower)\n",
        "        z_q, indices, kl, commit_loss = self.codebook(z_e)\n",
        "        z_e_lower_tilde = self.decoder(z_q)\n",
        "        return z_e_lower_tilde, z_e_lower, z_q, z_e, indices, kl, commit_loss\n",
        "   \n",
        "    def forward_full_stack(self, img):\n",
        "        z_e = self.encode(img)\n",
        "        z_q, indices, kl, commit_loss = self.codebook(z_e)\n",
        "        img_recon_dist = self.decode(z_q)\n",
        "        return img_recon_dist, img, z_q, z_e, indices, kl, commit_loss\n",
        "\n",
        "    def encode_lower(self, x):\n",
        "        if self.prev_model is None:\n",
        "            return x\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                z_e_lower = self.prev_model.encode(x)\n",
        "                z_e_lower = self.normalize(z_e_lower)\n",
        "            return z_e_lower\n",
        "\n",
        "    def encode(self, x):\n",
        "        with torch.no_grad():\n",
        "            z_e_lower = self.encode_lower(x)\n",
        "            z_e = self.encoder(z_e_lower)\n",
        "        return z_e\n",
        "        \n",
        "    def decode_lower(self, z_q_lower):\n",
        "        with torch.no_grad():\n",
        "            recon = self.prev_model.decode(z_q_lower)           \n",
        "        return recon\n",
        "\n",
        "    def decode(self, z_q):\n",
        "        with torch.no_grad():\n",
        "            if self.prev_model is not None:\n",
        "                z_e_u = self.normalize.unnorm(self.decoder(z_q))\n",
        "                z_q_lower_tilde = self.prev_model.quantize(z_e_u)\n",
        "                recon = self.decode_lower(z_q_lower_tilde)\n",
        "            else:\n",
        "                recon = self.decoder(z_q)\n",
        "        return recon\n",
        "\n",
        "    def quantize(self, z_e):\n",
        "        z_q, _ = self.codebook.quantize(z_e)\n",
        "        return z_q\n",
        "\n",
        "    def reconstruct_average(self, x, num_samples=10):\n",
        "        \"\"\"Average over stochastic edecodes\"\"\"\n",
        "        b, c, h, w = x.shape\n",
        "        result = torch.empty((num_samples, b, c, h, w)).to(device)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            result[i] = self.decode(self.quantize(self.encode(x)))\n",
        "        return result.mean(0)\n",
        "\n",
        "    def reconstruct(self, x):\n",
        "        return self.decode(self.quantize(self.encode(x)))\n",
        "    \n",
        "    def reconstruct_from_codes(self, codes):\n",
        "        return self.decode(self.codebook.lookup(codes))\n",
        "    \n",
        "    def reconstruct_from_z_e(self, z_e):\n",
        "        return self.decode(self.quantize(z_e))\n",
        "    \n",
        "    def recon_loss(self, orig, recon):\n",
        "        return F.mse_loss(orig, recon, reduction='none').sum(dim=(1,2,3)).mean()\n",
        "\n",
        "    def __len__(self):\n",
        "        i = 1\n",
        "        layer = self\n",
        "        while layer.prev_model is not None:\n",
        "            i += 1\n",
        "            layer = layer.prev_model\n",
        "        return i\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        max_layer = len(self) - 1\n",
        "        if idx > max_layer:\n",
        "            raise IndexError(\"layer does not exist\")\n",
        "\n",
        "        layer = self\n",
        "        for _ in range(max_layer - idx):\n",
        "            layer = layer.prev_model\n",
        "        return layer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPVnEfmMpPep"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM1eQAPcpPep"
      },
      "source": [
        "def show_recon(img, *models):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=len(models), figsize=(10 * len(models), 5))\n",
        "\n",
        "    if not isinstance(axes, np.ndarray):\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        model.eval()\n",
        "    \n",
        "   \n",
        "        img_ = img.unsqueeze(0)\n",
        "        recon = model.reconstruct(img_).squeeze()\n",
        "\n",
        "        padding = np.ones([img.shape[0], 64, 1])\n",
        "        output = np.hstack([\n",
        "                            double_swap(img.cpu()),\n",
        "                            double_swap(padding), \n",
        "                            double_swap(recon.cpu()),\n",
        "                            double_swap(padding),\n",
        "                            double_swap(np.abs((img-recon).cpu())),\n",
        "                            ])\n",
        "        # V1\n",
        "        # output = np.moveaxis(output, 0, -1)\n",
        "        # V2\n",
        "        axes[i].imshow( output, vmin=0, vmax=1)\n",
        "\n",
        "\n",
        "\n",
        "def get_bit_usage(indices):\n",
        "    \"\"\" Calculate bits used by latent space vs max possible \"\"\"\n",
        "    num_latents = indices.shape[0] * indices.shape[1] * indices.shape[2]\n",
        "    avg_probs = F.one_hot(indices).float().mean(dim=(0, 1, 2, 3))\n",
        "    highest_prob = torch.max(avg_probs)\n",
        "    bits = (-(avg_probs * torch.log2(avg_probs + 1e-10)).sum()) * num_latents\n",
        "    max_bits = math.log2(256) * num_latents\n",
        "    return bits, max_bits, highest_prob\n",
        "\n",
        "\n",
        "def decay_temp_linear(step, total_steps, temp_base, temp_min=0.001):\n",
        "    factor = 1.0 - (step/total_steps)\n",
        "    return temp_min + (temp_base - temp_min) * factor"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctGMkQKCpPeq"
      },
      "source": [
        "def get_loss_hqa(img, model, epoch, step, commit_threshold=0.6, log=None):\n",
        "    recon, orig, z_q, z_e, indices, KL, commit_loss = model(img)\n",
        "    recon_loss = model.recon_loss(orig, recon)\n",
        "    \n",
        "    # calculate loss\n",
        "    dims = np.prod(recon.shape[1:]) # orig_w * orig_h * num_channels\n",
        "    loss = recon_loss/dims + 0.001*KL/dims + 0.001*(commit_loss)/dims\n",
        "    \n",
        "    # logging    \n",
        "    if step % 20 == 0:\n",
        "        nll = recon_loss\n",
        "        elbo = -(nll + KL)  \n",
        "        distortion_bpd = nll / dims / np.log(2)\n",
        "        rate_bpd = KL / dims / np.log(2)\n",
        "        \n",
        "        bits, max_bits, highest_prob = get_bit_usage(indices)\n",
        "        bit_usage_frac = bits / max_bits\n",
        "        \n",
        "        time = datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "        log_line = f\"{time}, epoch={epoch}, step={step}, loss={loss:.5f}, distortion={distortion_bpd:.3f}, rate={rate_bpd:.3f}, -elbo={-elbo:.5f}, nll={nll:.5f}, KL={KL:.5f}, commit_loss={commit_loss:.5f}, bit_usage={bit_usage_frac:.5f}, highest_prob={highest_prob:.3f}, temp={model.codebook.temperature:.5f}\"\n",
        "        print(log_line)\n",
        "\n",
        "        if log is not None:\n",
        "            with open(log, \"a\") as logfile:\n",
        "                logfile.write(log_line + \"\\n\")\n",
        "                \n",
        "    return loss, indices\n",
        "\n",
        "\n",
        "def train(model, optimizer, scheduler, epochs, decay=True, log=None):\n",
        "    step = 0\n",
        "    model.train()\n",
        "    temp_base = model.codebook.temperature\n",
        "    code_count = torch.zeros(model.codebook.codebook_slots).to(device)\n",
        "    total_steps = len(dl_train)*epochs\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for x in dl_train:\n",
        "            x = x.to(device)\n",
        "            \n",
        "            # anneal temperature\n",
        "            if decay is True:\n",
        "                model.codebook.temperature = decay_temp_linear(step+1, total_steps, temp_base, temp_min=0.001) \n",
        "            \n",
        "            loss, indices = get_loss_hqa(x, model, epoch, step, log=log)\n",
        "                \n",
        "            # take training step    \n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()     \n",
        "                \n",
        "            # code reset every 20 steps\n",
        "            indices_onehot = F.one_hot(indices, num_classes=model.codebook.codebook_slots).float()\n",
        "            code_count = code_count + indices_onehot.sum(dim=(0, 1, 2, 3))\n",
        "            if step % 20 == 0:\n",
        "                with torch.no_grad():\n",
        "                    max_count, most_used_code = torch.max(code_count, dim=0)\n",
        "                    frac_usage = code_count / max_count\n",
        "                    z_q_most_used = model.codebook.lookup(most_used_code.view(1, 1, 1)).squeeze()\n",
        "\n",
        "                    min_frac_usage, min_used_code = torch.min(frac_usage, dim=0)\n",
        "                    if min_frac_usage < 0.03:\n",
        "                        print(f'reset code {min_used_code}')\n",
        "                        moved_code = z_q_most_used + torch.randn_like(z_q_most_used) / 100\n",
        "                        model.codebook.codebook[min_used_code] = moved_code\n",
        "                    code_count = torch.zeros_like(code_count)\n",
        "\n",
        "            step += 1\n",
        "        if epoch % 5 == 0:\n",
        "            for n in range(0, 5):\n",
        "                reshaped = test_x[n].reshape((3,64,64))\n",
        "                show_recon(reshaped, model)\n",
        "                plt.show();"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBNI1zmmpPer"
      },
      "source": [
        "def train_full_stack(root, exp_name, epochs=150, lr=4e-4):\n",
        "    \n",
        "    enc_hidden_sizes = [16, 16, 32, 64, 128]\n",
        "    dec_hidden_sizes = [16, 64, 256, 512, 1024]\n",
        "    \n",
        "    os.makedirs(root + \"/log\", exist_ok=True)\n",
        "    \n",
        "    for i in range(5):\n",
        "        print(f\"training layer{i}\")\n",
        "        if i == 0:\n",
        "            hqa = HQA.init_bottom(\n",
        "                input_feat_dim=3, # over here the number of initial channels (Grayscale = 1 , RGB = 3)\n",
        "                enc_hidden_dim=enc_hidden_sizes[i],\n",
        "                dec_hidden_dim=dec_hidden_sizes[i],\n",
        "            ).to(device)\n",
        "        else:\n",
        "            hqa = HQA.init_higher(\n",
        "                hqa_prev,\n",
        "                enc_hidden_dim=enc_hidden_sizes[i],\n",
        "                dec_hidden_dim=dec_hidden_sizes[i],\n",
        "            ).to(device)\n",
        "        \n",
        "        print(f\"layer{i} param count {sum(x.numel() for x in hqa.parameters()):,}\")\n",
        "        \n",
        "        log_file = f\"{root}/log/{exp_name}_l{i}.log\"\n",
        "        opt = RAdam(hqa.parameters(), lr=lr)\n",
        "        scheduler = FlatCA(opt, steps=epochs*len(dl_train), eta_min=lr/10)\n",
        "        train(hqa, opt, scheduler, epochs, log=log_file)\n",
        "        hqa_prev = hqa\n",
        "    \n",
        "    torch.save(hqa, f\"{root}/{exp_name}.pt\")\n",
        "    \n",
        "    return hqa"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yABvIzEYpPes"
      },
      "source": [
        "### Train HQA Stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kVO-BGlpPes",
        "scrolled": true,
        "outputId": "3de2f621-5a40-42f9-c837-b69dbc42a72f"
      },
      "source": [
        "# Train a HQA stack\n",
        "model_name = \"hqa_model\"\n",
        "models_dir = f\"{os.getcwd()}/models\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "if not os.path.isfile(f\"{models_dir}/{model_name}.pt\"):\n",
        "  hqa_model = train_full_stack(models_dir, model_name, epochs=150)\n",
        "else:\n",
        "  hqa_model = torch.load(f\"{models_dir}/{model_name}.pt\")\n",
        "\n",
        "hqa_model.eval()\n",
        "    \n",
        "layer_names = [\"Layer 0\", \"Layer 1\", \"Layer 2\", \"Layer 3\", \"Layer 4 Final\"]\n",
        "layer_descriptions = [\n",
        "    \"downsample 2 in each dimension, latent space size of 16x16\",\n",
        "    \"downsample 4 in each dimension, latent space size of 8x8\",\n",
        "    \"downsample 8 in each dimension, latent space size of 4x4\",\n",
        "    \"downsample 16 in each dimension, latent space size of 2x2\",\n",
        "    \"downsample 32 in each dimension, latent space size of 1x1\",\n",
        "]\n",
        "torch.save(hqa_model.state_dict(), f\"{models_dir}/{model_name}.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training layer0\n",
            "layer0 param count 29,475\n",
            "19/04/2021 14:38:36, epoch=0, step=0, loss=0.77245, distortion=1.109, rate=0.598, -elbo=14541.34082, nll=9446.59570, KL=5094.74512, commit_loss=40153.84375, bit_usage=0.00404, highest_prob=0.004, temp=0.66698\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: UserWarning: This overload of addcmul_ is deprecated:\n",
            "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "19/04/2021 14:38:47, epoch=0, step=20, loss=0.73844, distortion=1.060, rate=0.593, -elbo=14081.24316, nll=9028.69824, KL=5052.54492, commit_loss=40247.72266, bit_usage=0.00404, highest_prob=0.004, temp=0.66652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOh5BIzOpPex"
      },
      "source": [
        "### Layer Reconstructions\n",
        "Final layer recons match HQA recons from Table 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA73yED1pPey"
      },
      "source": [
        "def double_swap(img):\n",
        "  img = np.swapaxes(img, 0,-1)\n",
        "  img = np.swapaxes(img, 0,1)\n",
        "  return img\n",
        "\n",
        "def show_image(im_data, scale=10):\n",
        "    dpi = matplotlib.rcParams['figure.dpi']\n",
        "    width, height ,channel = im_data.shape\n",
        "    figsize = scale * width / float(dpi), scale * height / float(dpi)\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = fig.add_axes([0, 0, 1, 1])\n",
        "    # Hide spines, ticks, etc.\n",
        "    ax.axis('off')\n",
        "    ax.imshow(im_data, vmin=0, vmax=1)\n",
        "    plt.show();\n",
        "    ax.set(xlim=[0, width], ylim=[height, 0], aspect=1)\n",
        "def recon_comparison(model, names, descriptions, indexes=[0, 4, 15, 16, 18]):\n",
        "    images = []\n",
        "    for idx in indexes:\n",
        "        image = celeba_dataset[idx]    \n",
        "        img = image.to(device).squeeze()\n",
        "        img = double_swap(img.cpu().numpy())\n",
        "        images.append(img)\n",
        "    print(\"Original images to be reconstructed\")\n",
        "    output = np.hstack(images)\n",
        "\n",
        "    show_image(output,2)\n",
        "    \n",
        "    for layer, name, description in zip(model, names, descriptions):\n",
        "        images = []\n",
        "        \n",
        "        for idx in indexes:\n",
        "            image = celeba_dataset[idx]    \n",
        "            img = image.to(device).squeeze()\n",
        "            \n",
        "            for_recon = img.unsqueeze(0)\n",
        "            layer.eval()\n",
        "            recon = layer.reconstruct(for_recon).squeeze()\n",
        "            recon = double_swap(recon.cpu().numpy())\n",
        "            images.append(recon)\n",
        "        \n",
        "        print(f\"{name}: {description}\")\n",
        "        output = np.hstack(images)\n",
        "        show_image(output,2)\n",
        "# Show reconstruction comparison over each layer in HQA\n",
        "recon_comparison(hqa_model, layer_names, layer_descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg74Bli_pPez"
      },
      "source": [
        "### Distortions\n",
        "HQA distortions in Figure 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO1FllDzpPez"
      },
      "source": [
        "def get_rate_upper_bound(model, example_input):\n",
        "    assert len(example_input.shape) == 4, \"Expected (1, num_channels, x_h, x_w)\"\n",
        "    assert example_input.shape[0] == 1, \"Please provide example with batch_size=1\"\n",
        "    \n",
        "    z_e = model.encode(example_input)\n",
        "    _, top_indices, _, _ = model.codebook(z_e)\n",
        "        \n",
        "    # assume worst case scenario: we have a uniform usage of all our codes\n",
        "    rate_bound = top_indices[0].numel() * np.log2(model.codebook.codebook_slots)\n",
        "\n",
        "    return rate_bound\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    total_nll = []\n",
        "    total_kl = []\n",
        "    \n",
        "    for x in dl_train:\n",
        "        img = x.to(device)       \n",
        "        recon, orig, z_q, z_e, indices, kl, _ = model.forward_full_stack(img)       \n",
        "        recon_loss = model[0].recon_loss(img, recon)        \n",
        "        total_nll.append(recon_loss.item())\n",
        "        if kl != 0:\n",
        "            total_kl.append(kl.item())\n",
        "        else:\n",
        "            total_kl.append(kl)\n",
        "    \n",
        "    dims = np.prod(x.shape[1:])\n",
        "    kl_mean = np.mean(total_kl)\n",
        "    nll_mean = np.mean(total_nll)\n",
        "    distortion_bpd = nll_mean / dims / np.log(2)\n",
        "    rate_bpd = kl_mean / dims / np.log(2)\n",
        "    elbo = -(nll_mean + kl_mean)\n",
        "    \n",
        "    rate_bound = get_rate_upper_bound(model, img[0].unsqueeze(0))\n",
        "    \n",
        "    return distortion_bpd, rate_bound\n",
        "\n",
        "\n",
        "def get_rd_data(model):\n",
        "    dist = []\n",
        "    rates = []\n",
        "    \n",
        "    for i, _ in enumerate(model):\n",
        "        d, r = test(model[i])\n",
        "        dist.append(float(d))\n",
        "        rates.append(float(r))\n",
        "    \n",
        "    return dist, rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LmW-8bspPe0"
      },
      "source": [
        "# Layer distortions\n",
        "distortions, rates = get_rd_data(hqa_model)\n",
        "print(\"Name \\t\\t Distortion \\t Rate\")\n",
        "for dist, rate, name in zip(distortions, rates, layer_names):\n",
        "    print(f\"{name} \\t {dist:.4f} \\t {int(rate)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH3w5jLopPe1"
      },
      "source": [
        "### \"Free\" Samples: enumerating over all 1x1 latents\n",
        "Appendix Figure 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-4Z2tq3pPe1"
      },
      "source": [
        "num_codes = hqa_model.codebook.codebook_slots\n",
        "results = torch.Tensor(num_codes, 3, 32, 32).to(device)\n",
        "count=0\n",
        "for i in range(num_codes):\n",
        "    codes = torch.LongTensor([i]).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    results[count] = hqa_model.reconstruct_from_codes(codes)\n",
        "    count += 1\n",
        "        \n",
        "grid_img = make_grid(results.cpu(), nrow=16)\n",
        "grid_img = double_swap(grid_img)\n",
        "show_image(grid_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N010LoM4pPe2"
      },
      "source": [
        "### Final Layer Interpolations\n",
        "Appendix Figure 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_KmpQFNpPe2"
      },
      "source": [
        "grid_x = grid_y = 16\n",
        "results = torch.Tensor(grid_x * grid_y, 3,64, 64)\n",
        "i = 0\n",
        "\n",
        "for j in range(grid_y):\n",
        "    x_a = celeba_dataset[j]\n",
        "    x_b= celeba_dataset[j+grid_y]\n",
        "    point_1 = hqa_model.encode(x_a.unsqueeze(0).to(device)).cpu()\n",
        "    point_2 = hqa_model.encode(x_b.unsqueeze(0).to(device)).cpu()\n",
        "    interpolate_x = np.linspace(point_1[0], point_2[0], grid_x)\n",
        "\n",
        "    for z_e_interpolated in interpolate_x:\n",
        "        z_e_i = torch.Tensor(z_e_interpolated).unsqueeze(0).to(device)\n",
        "        z_q = hqa_model.quantize(z_e_i)\n",
        "        results[i] = hqa_model.decode(z_q).squeeze()\n",
        "        i += 1\n",
        "            \n",
        "grid_img = make_grid(results.cpu(), nrow=grid_x)\n",
        "grid_img = double_swap(grid_img)\n",
        "\n",
        "show_image(grid_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp7tjfFSpPe3"
      },
      "source": [
        "### Stochastic Reconstructions\n",
        "Appendix Figure 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_lIBJGvpPe3"
      },
      "source": [
        "# Show held-out reconstructions: [ORIG, 14xSAMPLE, AVERAGED_10_SAMPLES]\n",
        "grid_x = grid_y = 16\n",
        "results = torch.Tensor(grid_x * grid_y,3,64, 64)\n",
        "\n",
        "result_idx = 0\n",
        "for test_idx in range(grid_y):\n",
        "    x_a = celeba_dataset[test_idx]\n",
        "    img = x_a.squeeze().to(device)\n",
        "    img_ = img.unsqueeze(0)\n",
        "    num_examples = 5\n",
        "    \n",
        "    # ORIG\n",
        "    results[result_idx] = img\n",
        "    result_idx += 1\n",
        "    \n",
        "    # 14 RANDOM STOCHASTIC DECODES\n",
        "    for _ in range(grid_x -2):\n",
        "        results[result_idx] = hqa_model.reconstruct(img_)\n",
        "        result_idx += 1\n",
        "    \n",
        "    # AVERAGED SAMPLES\n",
        "    results[result_idx] = hqa_model.reconstruct_average(img_, num_samples=14).squeeze()\n",
        "    result_idx += 1\n",
        "\n",
        "grid_img = make_grid(results.cpu(), nrow=grid_x)\n",
        "grid_img = double_swap(grid_img)\n",
        "\n",
        "show_image(grid_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdHWbL7spPe4"
      },
      "source": [
        "### Layer-wise Interpolations\n",
        "Appendix Table 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWqqJYafpPe4"
      },
      "source": [
        "def interpolate(a, b, vqvae, grid_x=16):\n",
        "    images = []\n",
        "    \n",
        "    x_a = celeba_dataset[a]\n",
        "    x_b = celeba_dataset[b]\n",
        "    point_1 = vqvae.encode(x_a.unsqueeze(0).to(device))\n",
        "    point_2 = vqvae.encode(x_b.unsqueeze(0).to(device))\n",
        "\n",
        "    interpolate_x = np.linspace(point_1[0].cpu().numpy(), point_2[0].cpu().numpy(), grid_x)\n",
        "    \n",
        "    results = torch.Tensor(len(interpolate_x), 3,64, 64)\n",
        "    for i, z_e_interpolated in enumerate(interpolate_x):       \n",
        "        z_e = torch.Tensor(z_e_interpolated).unsqueeze(0).to(device)\n",
        "        z_q = vqvae.quantize(z_e)\n",
        "        recon = vqvae.decode(z_q).squeeze() \n",
        "        results[i] = recon\n",
        "\n",
        "    grid_img = make_grid(results.cpu(), nrow=grid_x)\n",
        "    grid_img = double_swap(grid_img)\n",
        "    show_image(grid_img)\n",
        "\n",
        "def show_original(idx):\n",
        "    x= celeba_dataset[idx]\n",
        "    image = x.squeeze()\n",
        "    image = double_swap(image)\n",
        "    show_image(image,2)\n",
        "    \n",
        "print(\"Originals\")\n",
        "show_original(1)\n",
        "show_original(9)\n",
        "for layer, name, description in zip(hqa_model, layer_names, layer_descriptions):\n",
        "    print(f\"{name} : {description}\")\n",
        "    interpolate(1, 9, layer, grid_x=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtydVk6YpPe5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}